{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torchvision import datasets, models, transforms\n",
    "import sys\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import os as os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, Shift\n",
    "from scipy.io.wavfile import read, write\n",
    "import soundfile as sf\n",
    "from librosa.display import specshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import soundfile as sf\n",
    "def wav_to_spec(path):\n",
    "    sr = 16000\n",
    "    wav, s = librosa.load(path)\n",
    "    if (len(wav) < sr):\n",
    "        wav = np.append(wav, np.zeros(sr - len(wav)))\n",
    "    elif (len(wav) > sr):\n",
    "        wav = wav[:sr]\n",
    "    #sf.write(path, wav, sr) \n",
    "    \n",
    "    # Generating Spectogram and saving as 'png'\n",
    "    plt.figure()\n",
    "    src_ft = librosa.stft(wav)\n",
    "    src_db = librosa.amplitude_to_db(abs(src_ft))\n",
    "    plt.axis('off')\n",
    "    specshow(src_db, sr=sr)  \n",
    "    plt.savefig(Path(path_spectrograms +'\\\\'+ str(path).split('\\\\')[-2]+ '\\\\'+ str(path).split('\\\\')[-1][:-3]+ 'png'))\n",
    "    plt.close()\n",
    "\n",
    "# walkover folders and get list of files\n",
    "def load_data(dir_list_walkout, dir_list_not_walkout):\n",
    "    for file in dir_list_walkout:\n",
    "        wav_to_spec(Path(path_walkout +\"\\\\\"+ file))\n",
    "    for file in dir_list_not_walkout:\n",
    "        wav_to_spec(Path(path_not_walkout +\"\\\\\"+ file))\n",
    "    \n",
    "\n",
    "# wav augmentation to increase audio datatset\n",
    "def augmentation_wav():\n",
    "    \n",
    "    augment = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.005, p=0.5),\n",
    "        PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "        Shift(min_fraction=-0.2, max_fraction=0.2, p=0.5),\n",
    "    ])\n",
    "\n",
    "    for file in dir_list_walkout:\n",
    "        for i in range(2):\n",
    "            samples, sr= librosa.load(Path(path_walkout +\"\\\\\"+ file))\n",
    "            augmented_samples = augment(samples=samples, sample_rate=sr)\n",
    "            write(Path(path_walkout +\"\\\\\"+ file[:-4]+\"{}.wav\".format(i)), sr, augmented_samples)\n",
    "\n",
    "    for file in dir_not_list_walkout:\n",
    "        samples, sr= librosa.load(Path(path_not_walkout +\"\\\\\"+ file))\n",
    "        augmented_samples = augment(samples=samples, sample_rate=sr)\n",
    "        write(Path(path_not_walkout +\"\\\\\"+ file[:-4]+\"{}.wav\".format(i)), sr, augmented_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of files\n",
    "path_walkout = r\"C:\\Users\\Ihtisham Ahmad\\Desktop\\ipython\\sterlex\\dataset\\walkout\"\n",
    "path_not_walkout = r\"C:\\Users\\Ihtisham Ahmad\\Desktop\\ipython\\sterlex\\dataset\\not walkout\"\n",
    "path_spectrograms = r\"C:\\Users\\Ihtisham Ahmad\\Desktop\\ipython\\sterlex\\dataset\\spectograms\"\n",
    "\n",
    "dir_list_walkout = os.listdir(path_walkout)\n",
    "dir_list_not_walkout = os.listdir(path_not_walkout)\n",
    "\n",
    "    \n",
    "files = dir_list_walkout + dir_list_not_walkout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 172\n",
      "    Root location: .\\dataset\\spectograms\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(201, 81), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# load saved spectograms data and apply transformations\n",
    "\n",
    "load_data(dir_list_walkout, dir_list_not_walkout)\n",
    "\n",
    "data_path = r'.\\dataset\\spectograms' #looking in subfolders\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()\n",
    "                                  ])\n",
    ")\n",
    "print(yes_no_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'not walkout': 0, 'walkout': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map = yes_no_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 137\n",
      "Testing size: 35\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(yes_no_train_dataset))\n",
    "print(\"Testing size:\",len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 70, 0: 67})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test dataloaders\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# CNN Architecture\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Augmentations for spectograms during training\n",
    "aug = transforms.Compose([torchaudio.transforms.FrequencyMasking(freq_mask_param=20),\n",
    "              torchaudio.transforms.TimeMasking(time_mask_param=20)])\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        X = aug(X)\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    best = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    acc = 100*correct\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "epochs = 200\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNet                                    [8, 2]                    --\n",
       "├─Conv2d: 1-1                            [8, 32, 197, 77]          2,432\n",
       "├─Conv2d: 1-2                            [8, 64, 94, 34]           51,264\n",
       "├─Dropout2d: 1-3                         [8, 64, 94, 34]           --\n",
       "├─Flatten: 1-4                           [8, 51136]                --\n",
       "├─Linear: 1-5                            [8, 50]                   2,556,850\n",
       "├─Linear: 1-6                            [8, 2]                    102\n",
       "==========================================================================================\n",
       "Total params: 2,610,648\n",
       "Trainable params: 2,610,648\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.63\n",
       "==========================================================================================\n",
       "Input size (MB): 1.56\n",
       "Forward/backward pass size (MB): 44.16\n",
       "Params size (MB): 10.44\n",
       "Estimated Total Size (MB): 56.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(16, 3, 201, 81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "class_map = ['not_walkout', 'walkout']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, Y) in enumerate(test_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        print(X.shape)\n",
    "        print(Y)\n",
    "        pred = model(X)\n",
    "\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[4].argmax(0),class_map[pred[4].argmax(0)]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(Y[4],class_map[Y[4]]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scripted_model = torch.jit.script(model)\n",
    "torch.jit.save(scripted_model, 'cnn_97_accuracy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
